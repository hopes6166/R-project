---
title: "Classification"
author: "Minh Ho"
date: "5/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
breast <- read.csv("/Users/apple/R studying/R Files For PA Exam/breast.csv")
names(breast)
summary(breast)
```
```{r}
 # Importing library
library(ggplot2)
library(caret)
library(dplyr)
library(rpart)
```

```{r}
breast$ID <- NULL
breast <- breast[!(breast$bNuclei == "?"), ]
breast$bNuclei <- factor(breast$bNuclei, levels = 1:10, order = TRUE)
breast$bNuclei <- as.integer(breast$bNuclei)
# Draw plot
for (i in Cat.var) {
  plot <- ggplot(data = breast, aes(x = factor(breast[ ,i])) + geom_bar() + labs(x = i)
  print(plot)
}



```

```{r}
library(rpart)
library(rpart.plot)
```

```{r}
breast$class <- as.factor(breast$class)
summary(breast)
```


```{r}
set.seed(2019)
partition <- createDataPartition(y = breast$class, p = .7, list =  FALSE)
train <- breast[partition, ]
test  <- breast[-partition, ]
table(train$class)/nrow(train)
table(test$class)/nrow(test)

```

```{r}
 # Draw plots to see the relationship between class and other independent variables
vars <- names(breast)[-10] # other way vars <- names(breast)[1: length(names(breast)) -1]
for (i in vars) {
  plot  <- ggplot(data =  train, aes(x = class, y = train[ ,i])) + geom_boxplot() + labs(y = i)
  print(plot)
}
```


We can see that the more close of the predictive feature to 10 the more probable it is to be maglinant.

```{r}
set.seed(123)
dt.1 <- rpart(class ~. , data = train, method = "class", control = rpart.control(minibucket = 2, cp = 0.0005,  maxdepth = 20),  parms = list(split = "gini"))
```

```{r}
dt.1
```

```{r}
rpart.plot(dt.1)
```
```{r}
dt.1$cptable
plotcp(dt.1)
```
We use the rule of 1 standard error from the most complex model, shown by the dotted line. We see that the least complex but still within 1 se from the most complex model is the model with 3 terminal node.

```{r}
dt.2 <- prune(dt.1, cp = dt.1$cptable[3, "CP"])
rpart.plot(dt.2)
```

```{r}
pred.1.class <- predict(dt.1, newdata = test, type = "class")
pred.2.class <- predict(dt.2, newdata = test, type = "class")
```

Then I create a confusion matrix

```{r}
confusionMatrix(pred.1.class, test$class,positive = "1")
```
```{r}
confusionMatrix(test$class, pred.2.class, positive = "1")
```
```{r}
library(pROC)
 # Extract the predicted probabilities for the second class
pred.1 <- predict(dt.1, newdata = test)[, 2]
pred.2 <- predict(dt.2, newdata = test)[, 2]
roc(test$class, pred.1, auc = TRUE)
```

```{r}
roc(test$class, pred.2, auc = TRUE)
```

ENSEMBLE TREE 1- RANDOM FOREST

```{r}
set.seed(1)
control <- trainControl(method = "repeatedCV",
                        number = 5,
                        repeats = 3,
                        sampling = "down")

rf.grid  <- expand.grid(mtry = 1:9)

```

```{r}
rf <- train(class~., 
            data = train, 
            method = "rf",
            ntrees = 200,
            importance = TRUE,
            trControl = control,
            tuneGrid = rf.grid)
```

```{r}
rf
```

```{r}
pred.rf.class <- predict(rf, newdata = test)
confusionMatrix(pred.rf.class, test$class, positive =  "1")
pred.rf.prob <- predict(rf, newdata = test, type = "prob")[, 2]
library(pROC)
roc(test$class, pred.rf.prob, auc = TRUE)
```

```{r}
imp <- varImp(rf)
plot(imp, main = "Variable Importance of Classification Random Forest")
```

ENSEMBLE TREE 2: BOOSTED MODEL

```{r}
  # CHUNK 17
breast$class <- ifelse(breast$class == 0, "B", "M")
breast$clas <- as.factor(breast.class)

#train test split
train <- breast[partition, ]
test <- breast[-partition, ]
```

```{r}
 # CHUNK 18
xgb.grid <- expand.grid(max_depth = c(1, 3, 7),
                        min_child_weight = 1,
                        gamma = 0,
                        nrounds  = 1000,
                        eta = c(0.01, 0.05, 0.1),
                        colsample_bytree = c(0.6, 0.9),
                        subsample = 0.6)

control <-  trainControl(method = "cv", 
                         number = 5, 
                         sampling = "down")

xgb.tuned <- train(class ~., 
                   data = train,
                   method = "xgbTree",
                   trControl = control,
                   tuneGrid = xgb.grid)

xgb.tuned
```
```{r}
pred.xgb.class <- predict(xgb.tuned, newdata = test)
confusionMatrix(pred.xgb.class, test$class, positive = "1")
pred.xgb.prob  <- predict(xgb.tuned, newdata = test, type = "prob")[, 2]
roc(test$class, pred.xgb.prob, auc = TRUE)
```

